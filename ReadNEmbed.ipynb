{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering_tool.embedders.bert import *\n",
    "from clustering_tool.embedders.word2vec import *\n",
    "from clustering_tool.embedders.common import *\n",
    "from clustering_tool.embedders.tfidf import *\n",
    "from clustering_tool.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertTokenizer = create_tokenizer()\n",
    "bertModel = create_model()\n",
    "\n",
    "def get_file_paths(dataset_name, emb_name):\n",
    "    input_path = 'data/{}.txt'.format(dataset_name)\n",
    "    output_path = 'data/{}_{}.npy'.format(dataset_name, emb_name)\n",
    "    return input_path, output_path\n",
    "\n",
    "def tf_idf(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'tf_idf')\n",
    "    tfidf_embedder = TfIdfEmbedder(tfidf_vectorizer(input_path))\n",
    "    read_n_save(input_path, output_path, embedder=tfidf_embedder)\n",
    "    \n",
    "    \n",
    "def word2vec_md_mean(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'word2vec_md_avg')\n",
    "    word2vecEmbedder = Word2VecEmbedder('en_core_web_md')\n",
    "    read_n_save(input_path, output_path, embedder=word2vecEmbedder)\n",
    "    \n",
    "def word2vec_lg_mean(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'word2vec_lg_avg')\n",
    "    word2vecEmbedder = Word2VecEmbedder('en_core_web_lg')\n",
    "    read_n_save(input_path, output_path, embedder=word2vecEmbedder)\n",
    "    \n",
    "def word2vec_md_weighted(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'word2vec_md_weighted')\n",
    "    spacyModel=spacy.load('en_core_web_md')\n",
    "    wordFreq = get_unigrams(input_path, SpacyTokenizer(spacyModel.tokenizer))\n",
    "    word2vecEmbedder = Word2VecEmbedder('en_core_web_md', WeightedAverageEmbeddings(wordFreq))\n",
    "    read_n_save(input_path, output_path, embedder=word2vecEmbedder)\n",
    "    \n",
    "def word2vec_lg_weighted(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'word2vec_lg_weighted')\n",
    "    spacyModel=spacy.load('en_core_web_lg')\n",
    "    wordFreq = get_unigrams(input_path, SpacyTokenizer(spacyModel.tokenizer))\n",
    "    word2vecEmbedder = Word2VecEmbedder('en_core_web_lg', WeightedAverageEmbeddings(wordFreq))\n",
    "    read_n_save(input_path, output_path, embedder=word2vecEmbedder)\n",
    "    \n",
    "def bert_mean(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'bert_avg')\n",
    "    bertEmbedder = BertEmbedder(bertTokenizer, bertModel, device=device, embedding_strategy=bert_avg_embeddings)\n",
    "    read_n_save(input_path, output_path, embedder=bertEmbedder)\n",
    "    \n",
    "def bert_cls(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'bert_cls')\n",
    "    bertEmbedder = BertEmbedder(bertTokenizer, bertModel, device=device, embedding_strategy=bert_cls_embeddings)\n",
    "    read_n_save(input_path, output_path, embedder=bertEmbedder)\n",
    "    \n",
    "def bert_pooler(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'bert_pooler')\n",
    "    bertEmbedder = BertEmbedder(bertTokenizer, bertModel, device=device, embedding_strategy=bert_pooler_embeddings)\n",
    "    read_n_save(input_path, output_path, embedder=bertEmbedder)\n",
    "    \n",
    "def bert_weighted(dataset_name):\n",
    "    input_path, output_path = get_file_paths(dataset_name, 'bert_pooler')\n",
    "    unigram=get_unigrams(input_path, bertTokenizer)\n",
    "    bertEmbedder = BertEmbedder(bertTokenizer, bertModel, device=device, embedding_strategy=BertWeightedEmbeddings(unigram, device=device))\n",
    "    read_n_save(input_path, output_path, embedder=bertEmbedder)\n",
    "    \n",
    "def all_embeds(dataset_name):\n",
    "    tf_idf(dataset_name)\n",
    "    word2vec_md_mean(dataset_name)\n",
    "    word2vec_lg_mean(dataset_name)\n",
    "    word2vec_md_weighted(dataset_name)\n",
    "    word2vec_lg_weighted(dataset_name)\n",
    "    bert_mean(dataset_name)\n",
    "    bert_cls(dataset_name)\n",
    "    bert_pooler(dataset_name)\n",
    "    bert_weighted(dataset_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['SearchSnippets', 'Biomedical', 'StackOverflow']\n",
    "for dataset in datasets:\n",
    "    all_embeds(dataset)\n",
    "    \n",
    "    labels = []\n",
    "    with open('data/{}_gnd.txt') as label_file:\n",
    "        for line in label_file:\n",
    "            labels.append(int(line.strip()))\n",
    "            \n",
    "    np.save('data/{}_label.npy', np.array(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
