{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from joblib import dump, load\n",
    "import torch\n",
    "import numpy\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from clustering_tool.model import DeepClusteringModel\n",
    "from clustering_tool.modules.clusterers.XieClusterer import XieClusterer\n",
    "from clustering_tool.modules.embedders.PcaEmbedder import PcaEmbedder\n",
    "from clustering_tool.modules.encoders.feedforward import MyFeedForward\n",
    "from clustering_tool.modules.metrics.NormalizedMutualInformation import NormalizedMutualInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_rcv1(data_home='../data', subset=\"train\", shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23149, 47236)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(2000)\n",
    "#pca.fit(dataset.data.toarray())\n",
    "#dump(pca, '../reuters_pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6768940886439831"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = load('../reuters_pca.joblib')\n",
    "pca.transform(dataset.data[0:1000].toarray()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump(pca, '../data/reuters_pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(2000, 200)\n",
    "dropout = torch.nn.Dropout(p=0.0)\n",
    "activation = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = PcaEmbedder(\"../data/reuters_pca.joblib\")\n",
    "embedder.requires_grad_(False)\n",
    "encoder = MyFeedForward(2000, 1, 200, torch.nn.Sigmoid())\n",
    "clusterer = XieClusterer(103, 200)\n",
    "\n",
    "model = DeepClusteringModel(None, encoder, clusterer, 103, embedder = embedder)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(dataset.data[0:2].toarray(), dtype=torch.float)\n",
    "y = torch.tensor(dataset.target[0:2].toarray(), dtype=torch.float)\n",
    "#x = torch.Tensor(pca.transform(dataset.data[0:10].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': tensor([[0.5029, 0.5021, 0.4950, 0.4973, 0.5033, 0.5004, 0.4998, 0.5036, 0.4995,\n",
       "          0.4967, 0.4971, 0.5041, 0.5047, 0.5002, 0.5034, 0.5018, 0.5072, 0.5058,\n",
       "          0.5054, 0.4960, 0.4987, 0.4971, 0.4963, 0.5039, 0.5031, 0.4912, 0.4978,\n",
       "          0.4987, 0.5012, 0.5042, 0.4992, 0.4920, 0.4959, 0.4930, 0.5040, 0.5021,\n",
       "          0.5068, 0.5073, 0.4968, 0.5014, 0.4986, 0.4971, 0.4952, 0.5000, 0.5009,\n",
       "          0.4984, 0.5016, 0.4972, 0.5026, 0.5018, 0.5048, 0.5064, 0.5072, 0.4987,\n",
       "          0.4990, 0.4961, 0.4913, 0.5020, 0.5056, 0.5012, 0.4993, 0.4972, 0.5014,\n",
       "          0.4923, 0.4986, 0.4952, 0.5064, 0.5015, 0.4945, 0.4945, 0.4957, 0.5011,\n",
       "          0.4932, 0.4915, 0.4981, 0.5063, 0.4994, 0.4957, 0.4992, 0.5013, 0.4965,\n",
       "          0.5020, 0.5024, 0.4969, 0.5038, 0.4982, 0.4946, 0.4954, 0.5066, 0.4950,\n",
       "          0.4951, 0.4925, 0.5018, 0.4969, 0.4995, 0.5023, 0.5037, 0.5006, 0.5008,\n",
       "          0.4945, 0.5067, 0.4989, 0.4993, 0.5000, 0.4988, 0.4995, 0.5059, 0.4996,\n",
       "          0.5004, 0.5060, 0.5073, 0.5006, 0.5009, 0.4970, 0.5016, 0.5015, 0.5041,\n",
       "          0.5028, 0.4956, 0.5022, 0.5031, 0.4986, 0.4942, 0.5020, 0.4956, 0.5046,\n",
       "          0.5075, 0.5011, 0.4980, 0.5005, 0.5065, 0.4971, 0.5052, 0.5033, 0.4966,\n",
       "          0.4968, 0.5005, 0.4927, 0.4955, 0.4966, 0.5082, 0.5011, 0.5047, 0.5055,\n",
       "          0.4920, 0.5068, 0.4901, 0.4973, 0.4923, 0.5030, 0.5050, 0.4962, 0.4989,\n",
       "          0.5035, 0.4932, 0.5035, 0.5039, 0.4997, 0.4948, 0.4945, 0.4965, 0.5018,\n",
       "          0.4972, 0.4906, 0.4987, 0.4995, 0.4986, 0.5024, 0.5039, 0.5056, 0.5047,\n",
       "          0.4973, 0.4931, 0.4962, 0.5003, 0.4952, 0.5008, 0.5016, 0.4979, 0.5042,\n",
       "          0.4936, 0.4970, 0.4958, 0.5059, 0.4976, 0.5028, 0.5080, 0.5035, 0.4916,\n",
       "          0.5037, 0.4996, 0.4945, 0.5015, 0.4930, 0.4979, 0.4945, 0.5061, 0.4962,\n",
       "          0.4966, 0.4938],\n",
       "         [0.5052, 0.5014, 0.4940, 0.5020, 0.5036, 0.5025, 0.5028, 0.5040, 0.5037,\n",
       "          0.5002, 0.4965, 0.4989, 0.5019, 0.5020, 0.4967, 0.5009, 0.5041, 0.5061,\n",
       "          0.5060, 0.4984, 0.4994, 0.5064, 0.4936, 0.4986, 0.5071, 0.4965, 0.4984,\n",
       "          0.5007, 0.5016, 0.5012, 0.4910, 0.4924, 0.4965, 0.4899, 0.5035, 0.4980,\n",
       "          0.5059, 0.5050, 0.4938, 0.4945, 0.5024, 0.5003, 0.4972, 0.5003, 0.4954,\n",
       "          0.4966, 0.5024, 0.5013, 0.4990, 0.5026, 0.5071, 0.5038, 0.4985, 0.4990,\n",
       "          0.5014, 0.5009, 0.4964, 0.5012, 0.5095, 0.5029, 0.5020, 0.4966, 0.4993,\n",
       "          0.4935, 0.4987, 0.4970, 0.5003, 0.5064, 0.4994, 0.4974, 0.4971, 0.4956,\n",
       "          0.4903, 0.4936, 0.4978, 0.5061, 0.4942, 0.4932, 0.4971, 0.5025, 0.4912,\n",
       "          0.5025, 0.5050, 0.4950, 0.5010, 0.5017, 0.4973, 0.5006, 0.5056, 0.4942,\n",
       "          0.4952, 0.4967, 0.4973, 0.4946, 0.5011, 0.5025, 0.5074, 0.5036, 0.5005,\n",
       "          0.4987, 0.5040, 0.4909, 0.4997, 0.5038, 0.5067, 0.5001, 0.5008, 0.4939,\n",
       "          0.5028, 0.4992, 0.5078, 0.5009, 0.4960, 0.4971, 0.4995, 0.5038, 0.5024,\n",
       "          0.5043, 0.4964, 0.5012, 0.5034, 0.5020, 0.4928, 0.4990, 0.4934, 0.5017,\n",
       "          0.5078, 0.5023, 0.4970, 0.5043, 0.4997, 0.4950, 0.4997, 0.5013, 0.4938,\n",
       "          0.5001, 0.5044, 0.4909, 0.4953, 0.4963, 0.5035, 0.5039, 0.5015, 0.5068,\n",
       "          0.4964, 0.5067, 0.4927, 0.5022, 0.4987, 0.5042, 0.5070, 0.4976, 0.4989,\n",
       "          0.5009, 0.4957, 0.4958, 0.5060, 0.4989, 0.4927, 0.4977, 0.4948, 0.5025,\n",
       "          0.5022, 0.4963, 0.4979, 0.5014, 0.5004, 0.5074, 0.5024, 0.5028, 0.5059,\n",
       "          0.4930, 0.4980, 0.4967, 0.4953, 0.4990, 0.5033, 0.5007, 0.4998, 0.5029,\n",
       "          0.5004, 0.4990, 0.4930, 0.5046, 0.4959, 0.5035, 0.5090, 0.5035, 0.4980,\n",
       "          0.5022, 0.5024, 0.4956, 0.5019, 0.4955, 0.4958, 0.5012, 0.4980, 0.4952,\n",
       "          0.5023, 0.4962]], grad_fn=<SigmoidBackward>),\n",
       " 's': tensor([[0.0104, 0.0103, 0.0097, 0.0093, 0.0099, 0.0095, 0.0098, 0.0093, 0.0100,\n",
       "          0.0101, 0.0090, 0.0099, 0.0093, 0.0103, 0.0097, 0.0099, 0.0100, 0.0095,\n",
       "          0.0092, 0.0102, 0.0102, 0.0101, 0.0095, 0.0094, 0.0100, 0.0096, 0.0101,\n",
       "          0.0099, 0.0090, 0.0095, 0.0105, 0.0095, 0.0092, 0.0096, 0.0088, 0.0095,\n",
       "          0.0094, 0.0098, 0.0098, 0.0094, 0.0099, 0.0099, 0.0095, 0.0098, 0.0095,\n",
       "          0.0096, 0.0097, 0.0094, 0.0089, 0.0094, 0.0099, 0.0092, 0.0093, 0.0098,\n",
       "          0.0104, 0.0096, 0.0095, 0.0093, 0.0087, 0.0099, 0.0095, 0.0090, 0.0097,\n",
       "          0.0106, 0.0101, 0.0097, 0.0091, 0.0105, 0.0093, 0.0105, 0.0092, 0.0099,\n",
       "          0.0093, 0.0104, 0.0098, 0.0098, 0.0094, 0.0100, 0.0107, 0.0088, 0.0094,\n",
       "          0.0097, 0.0094, 0.0103, 0.0098, 0.0096, 0.0095, 0.0098, 0.0101, 0.0105,\n",
       "          0.0093, 0.0095, 0.0100, 0.0102, 0.0095, 0.0089, 0.0095, 0.0096, 0.0101,\n",
       "          0.0105, 0.0094, 0.0104, 0.0105],\n",
       "         [0.0104, 0.0103, 0.0097, 0.0093, 0.0099, 0.0095, 0.0098, 0.0093, 0.0100,\n",
       "          0.0101, 0.0090, 0.0099, 0.0093, 0.0103, 0.0097, 0.0099, 0.0100, 0.0095,\n",
       "          0.0092, 0.0102, 0.0102, 0.0101, 0.0095, 0.0094, 0.0100, 0.0096, 0.0101,\n",
       "          0.0099, 0.0090, 0.0095, 0.0105, 0.0095, 0.0092, 0.0097, 0.0088, 0.0095,\n",
       "          0.0094, 0.0098, 0.0098, 0.0094, 0.0099, 0.0099, 0.0095, 0.0098, 0.0095,\n",
       "          0.0096, 0.0097, 0.0094, 0.0089, 0.0094, 0.0099, 0.0092, 0.0093, 0.0098,\n",
       "          0.0104, 0.0096, 0.0095, 0.0093, 0.0087, 0.0099, 0.0095, 0.0090, 0.0097,\n",
       "          0.0106, 0.0101, 0.0097, 0.0091, 0.0105, 0.0093, 0.0105, 0.0092, 0.0099,\n",
       "          0.0093, 0.0105, 0.0098, 0.0098, 0.0094, 0.0100, 0.0107, 0.0088, 0.0094,\n",
       "          0.0097, 0.0094, 0.0103, 0.0098, 0.0096, 0.0095, 0.0098, 0.0101, 0.0105,\n",
       "          0.0093, 0.0095, 0.0100, 0.0102, 0.0095, 0.0089, 0.0095, 0.0096, 0.0102,\n",
       "          0.0105, 0.0094, 0.0104, 0.0105]], grad_fn=<DivBackward0>),\n",
       " 'loss': tensor(4.6434, grad_fn=<NegBackward>)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x, y) # Все дело в эмбедере"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "model.forward(x, y)['loss'].backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h': tensor([[0.5029, 0.5021, 0.4950, 0.4973, 0.5033, 0.5004, 0.4998, 0.5036, 0.4995,\n",
       "          0.4967, 0.4971, 0.5041, 0.5047, 0.5002, 0.5034, 0.5018, 0.5072, 0.5058,\n",
       "          0.5054, 0.4960, 0.4987, 0.4971, 0.4963, 0.5039, 0.5031, 0.4912, 0.4978,\n",
       "          0.4987, 0.5012, 0.5042, 0.4992, 0.4920, 0.4959, 0.4930, 0.5040, 0.5021,\n",
       "          0.5068, 0.5073, 0.4968, 0.5014, 0.4986, 0.4971, 0.4952, 0.5000, 0.5009,\n",
       "          0.4984, 0.5016, 0.4972, 0.5026, 0.5018, 0.5048, 0.5064, 0.5072, 0.4987,\n",
       "          0.4990, 0.4961, 0.4913, 0.5020, 0.5056, 0.5012, 0.4993, 0.4972, 0.5014,\n",
       "          0.4923, 0.4986, 0.4952, 0.5064, 0.5015, 0.4945, 0.4945, 0.4957, 0.5011,\n",
       "          0.4932, 0.4915, 0.4981, 0.5063, 0.4994, 0.4957, 0.4992, 0.5013, 0.4965,\n",
       "          0.5020, 0.5024, 0.4969, 0.5038, 0.4982, 0.4946, 0.4954, 0.5066, 0.4950,\n",
       "          0.4951, 0.4925, 0.5018, 0.4969, 0.4995, 0.5023, 0.5037, 0.5006, 0.5008,\n",
       "          0.4945, 0.5067, 0.4989, 0.4993, 0.5000, 0.4988, 0.4995, 0.5059, 0.4996,\n",
       "          0.5004, 0.5060, 0.5073, 0.5006, 0.5009, 0.4970, 0.5016, 0.5015, 0.5041,\n",
       "          0.5028, 0.4956, 0.5022, 0.5030, 0.4986, 0.4942, 0.5020, 0.4956, 0.5046,\n",
       "          0.5075, 0.5011, 0.4980, 0.5005, 0.5065, 0.4971, 0.5052, 0.5033, 0.4966,\n",
       "          0.4968, 0.5005, 0.4927, 0.4955, 0.4966, 0.5082, 0.5011, 0.5047, 0.5055,\n",
       "          0.4920, 0.5068, 0.4901, 0.4973, 0.4923, 0.5030, 0.5050, 0.4962, 0.4989,\n",
       "          0.5035, 0.4932, 0.5035, 0.5039, 0.4997, 0.4948, 0.4945, 0.4965, 0.5018,\n",
       "          0.4972, 0.4906, 0.4987, 0.4995, 0.4986, 0.5024, 0.5039, 0.5056, 0.5047,\n",
       "          0.4973, 0.4931, 0.4962, 0.5003, 0.4952, 0.5008, 0.5016, 0.4979, 0.5042,\n",
       "          0.4936, 0.4970, 0.4958, 0.5059, 0.4976, 0.5028, 0.5080, 0.5035, 0.4916,\n",
       "          0.5037, 0.4996, 0.4945, 0.5015, 0.4930, 0.4979, 0.4945, 0.5061, 0.4962,\n",
       "          0.4966, 0.4938],\n",
       "         [0.5052, 0.5014, 0.4940, 0.5020, 0.5036, 0.5025, 0.5028, 0.5040, 0.5037,\n",
       "          0.5002, 0.4965, 0.4989, 0.5019, 0.5020, 0.4967, 0.5009, 0.5041, 0.5061,\n",
       "          0.5060, 0.4984, 0.4994, 0.5064, 0.4936, 0.4986, 0.5071, 0.4965, 0.4984,\n",
       "          0.5007, 0.5016, 0.5012, 0.4910, 0.4924, 0.4965, 0.4899, 0.5035, 0.4980,\n",
       "          0.5059, 0.5050, 0.4938, 0.4945, 0.5024, 0.5003, 0.4972, 0.5003, 0.4954,\n",
       "          0.4966, 0.5024, 0.5013, 0.4990, 0.5026, 0.5071, 0.5038, 0.4985, 0.4990,\n",
       "          0.5014, 0.5009, 0.4964, 0.5012, 0.5095, 0.5029, 0.5020, 0.4966, 0.4993,\n",
       "          0.4935, 0.4987, 0.4970, 0.5003, 0.5064, 0.4994, 0.4974, 0.4971, 0.4956,\n",
       "          0.4903, 0.4936, 0.4978, 0.5061, 0.4942, 0.4932, 0.4971, 0.5025, 0.4912,\n",
       "          0.5025, 0.5050, 0.4950, 0.5010, 0.5017, 0.4973, 0.5006, 0.5056, 0.4942,\n",
       "          0.4952, 0.4967, 0.4973, 0.4945, 0.5011, 0.5025, 0.5074, 0.5036, 0.5005,\n",
       "          0.4987, 0.5040, 0.4909, 0.4997, 0.5038, 0.5067, 0.5001, 0.5008, 0.4939,\n",
       "          0.5028, 0.4992, 0.5078, 0.5009, 0.4960, 0.4971, 0.4995, 0.5038, 0.5024,\n",
       "          0.5043, 0.4964, 0.5012, 0.5034, 0.5020, 0.4928, 0.4990, 0.4934, 0.5017,\n",
       "          0.5078, 0.5023, 0.4970, 0.5043, 0.4997, 0.4950, 0.4997, 0.5013, 0.4938,\n",
       "          0.5001, 0.5044, 0.4909, 0.4953, 0.4963, 0.5035, 0.5039, 0.5015, 0.5068,\n",
       "          0.4964, 0.5067, 0.4927, 0.5022, 0.4987, 0.5042, 0.5070, 0.4976, 0.4989,\n",
       "          0.5009, 0.4957, 0.4958, 0.5060, 0.4989, 0.4927, 0.4977, 0.4948, 0.5025,\n",
       "          0.5022, 0.4963, 0.4979, 0.5014, 0.5004, 0.5074, 0.5024, 0.5028, 0.5059,\n",
       "          0.4930, 0.4980, 0.4967, 0.4953, 0.4990, 0.5033, 0.5007, 0.4998, 0.5029,\n",
       "          0.5004, 0.4990, 0.4930, 0.5046, 0.4959, 0.5035, 0.5090, 0.5035, 0.4979,\n",
       "          0.5022, 0.5024, 0.4956, 0.5019, 0.4955, 0.4958, 0.5012, 0.4980, 0.4952,\n",
       "          0.5023, 0.4962]], grad_fn=<SigmoidBackward>),\n",
       " 's': tensor([[0.0104, 0.0103, 0.0097, 0.0093, 0.0099, 0.0095, 0.0098, 0.0093, 0.0100,\n",
       "          0.0101, 0.0090, 0.0099, 0.0093, 0.0103, 0.0097, 0.0099, 0.0100, 0.0095,\n",
       "          0.0092, 0.0102, 0.0102, 0.0101, 0.0095, 0.0094, 0.0100, 0.0096, 0.0101,\n",
       "          0.0099, 0.0090, 0.0095, 0.0105, 0.0095, 0.0092, 0.0096, 0.0088, 0.0095,\n",
       "          0.0094, 0.0098, 0.0098, 0.0094, 0.0099, 0.0099, 0.0095, 0.0098, 0.0095,\n",
       "          0.0096, 0.0097, 0.0094, 0.0089, 0.0094, 0.0099, 0.0092, 0.0093, 0.0098,\n",
       "          0.0104, 0.0096, 0.0095, 0.0093, 0.0087, 0.0099, 0.0095, 0.0090, 0.0097,\n",
       "          0.0106, 0.0101, 0.0097, 0.0091, 0.0105, 0.0093, 0.0105, 0.0092, 0.0099,\n",
       "          0.0093, 0.0104, 0.0098, 0.0098, 0.0094, 0.0100, 0.0107, 0.0088, 0.0094,\n",
       "          0.0097, 0.0094, 0.0103, 0.0098, 0.0096, 0.0095, 0.0098, 0.0101, 0.0105,\n",
       "          0.0093, 0.0095, 0.0100, 0.0102, 0.0095, 0.0089, 0.0095, 0.0096, 0.0101,\n",
       "          0.0105, 0.0094, 0.0104, 0.0105],\n",
       "         [0.0104, 0.0103, 0.0097, 0.0093, 0.0099, 0.0095, 0.0098, 0.0093, 0.0100,\n",
       "          0.0101, 0.0090, 0.0099, 0.0093, 0.0103, 0.0097, 0.0099, 0.0100, 0.0095,\n",
       "          0.0092, 0.0102, 0.0102, 0.0101, 0.0095, 0.0094, 0.0100, 0.0096, 0.0101,\n",
       "          0.0099, 0.0090, 0.0095, 0.0105, 0.0095, 0.0092, 0.0097, 0.0088, 0.0095,\n",
       "          0.0094, 0.0098, 0.0098, 0.0094, 0.0099, 0.0099, 0.0095, 0.0098, 0.0095,\n",
       "          0.0096, 0.0097, 0.0094, 0.0089, 0.0094, 0.0099, 0.0092, 0.0093, 0.0098,\n",
       "          0.0104, 0.0096, 0.0095, 0.0093, 0.0087, 0.0099, 0.0095, 0.0090, 0.0097,\n",
       "          0.0106, 0.0101, 0.0097, 0.0091, 0.0105, 0.0093, 0.0105, 0.0092, 0.0099,\n",
       "          0.0093, 0.0105, 0.0098, 0.0098, 0.0094, 0.0100, 0.0107, 0.0088, 0.0094,\n",
       "          0.0097, 0.0094, 0.0103, 0.0098, 0.0096, 0.0095, 0.0098, 0.0101, 0.0105,\n",
       "          0.0093, 0.0095, 0.0100, 0.0102, 0.0095, 0.0089, 0.0095, 0.0096, 0.0102,\n",
       "          0.0105, 0.0094, 0.0104, 0.0105]], grad_fn=<DivBackward0>),\n",
       " 'loss': tensor(4.6434, grad_fn=<NegBackward>)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PcaEmbedder()]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(embedder.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14],\n",
       "        [15]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [2, 2, 3]])\n",
    "b = torch.tensor([1, 2, 3]).unsqueeze(-1)\n",
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052e-19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1e-20) * torch.log(torch.tensor(1e-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cl = 10\n",
    "nmi = NormalizedMutualInformation(n_cl, n_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0048186779022217"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = numpy.random.random((5, n_cl))\n",
    "y = numpy.random.random((5, n_cl))\n",
    "\n",
    "y = numpy.zeros_like(x)\n",
    "y_classes = numpy.argmax(x, axis=1)\n",
    "for i, cl in enumerate(y_classes):\n",
    "    y[i][cl] = 1\n",
    "\n",
    "x = normalize(x, axis=1, norm='l1')\n",
    "y = normalize(y, axis=1, norm='l1')\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "nmi(x, x)\n",
    "nmi.get_metric(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0700, 0.0000, 0.0153, 0.0000, 0.0000, 0.0176, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0528, 0.0000, 0.0116, 0.0000, 0.0000, 0.0229, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.1210, 0.0000, 0.0099, 0.0000, 0.0000, 0.0140, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0626, 0.0000, 0.0140, 0.0000, 0.0000, 0.0270, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0275, 0.0000, 0.0315, 0.0000, 0.0000, 0.0286, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0595, 0.0000, 0.0198, 0.0000, 0.0000, 0.0144, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0318, 0.0000, 0.0239, 0.0000, 0.0000, 0.0203, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0718, 0.0000, 0.0249, 0.0000, 0.0000, 0.0301, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0353, 0.0000, 0.0213, 0.0000, 0.0000, 0.0132, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0679, 0.0000, 0.0279, 0.0000, 0.0000, 0.0120, 0.0000,\n",
       "         0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_distr = torch.matmul(torch.t(x), y)\n",
    "joint_distr /= torch.sum(joint_distr)\n",
    "joint_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.sum(x, axis=0)\n",
    "x /= torch.sum(x)\n",
    "y = torch.sum(y, axis=0)\n",
    "y /= torch.sum(y)\n",
    "y[y < 1e-12]=1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0287e-13, 1.0287e-13, 6.1720e-02, 1.0287e-13, 2.0573e-02, 1.0287e-13,\n",
       "         1.0287e-13, 2.0573e-02, 1.0287e-13, 1.0287e-13],\n",
       "        [8.7250e-14, 8.7250e-14, 5.2350e-02, 8.7250e-14, 1.7450e-02, 8.7250e-14,\n",
       "         8.7250e-14, 1.7450e-02, 8.7250e-14, 8.7250e-14],\n",
       "        [1.4483e-13, 1.4483e-13, 8.6895e-02, 1.4483e-13, 2.8965e-02, 1.4483e-13,\n",
       "         1.4483e-13, 2.8965e-02, 1.4483e-13, 1.4483e-13],\n",
       "        [1.0364e-13, 1.0364e-13, 6.2182e-02, 1.0364e-13, 2.0727e-02, 1.0364e-13,\n",
       "         1.0364e-13, 2.0727e-02, 1.0364e-13, 1.0364e-13],\n",
       "        [8.7540e-14, 8.7540e-14, 5.2524e-02, 8.7540e-14, 1.7508e-02, 8.7540e-14,\n",
       "         8.7540e-14, 1.7508e-02, 8.7540e-14, 8.7540e-14],\n",
       "        [9.3612e-14, 9.3612e-14, 5.6167e-02, 9.3612e-14, 1.8722e-02, 9.3612e-14,\n",
       "         9.3612e-14, 1.8722e-02, 9.3612e-14, 9.3612e-14],\n",
       "        [7.5920e-14, 7.5920e-14, 4.5552e-02, 7.5920e-14, 1.5184e-02, 7.5920e-14,\n",
       "         7.5920e-14, 1.5184e-02, 7.5920e-14, 7.5920e-14],\n",
       "        [1.2677e-13, 1.2677e-13, 7.6062e-02, 1.2677e-13, 2.5354e-02, 1.2677e-13,\n",
       "         1.2677e-13, 2.5354e-02, 1.2677e-13, 1.2677e-13],\n",
       "        [6.9730e-14, 6.9730e-14, 4.1838e-02, 6.9730e-14, 1.3946e-02, 6.9730e-14,\n",
       "         6.9730e-14, 1.3946e-02, 6.9730e-14, 6.9730e-14],\n",
       "        [1.0785e-13, 1.0785e-13, 6.4709e-02, 1.0785e-13, 2.1570e-02, 1.0785e-13,\n",
       "         1.0785e-13, 2.1570e-02, 1.0785e-13, 1.0785e-13]], dtype=torch.float64)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_distr = torch.matmul(x.unsqueeze(-1), y.unsqueeze(0))\n",
    "xy_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0145, dtype=torch.float64)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.kl_div(joint_distr, xy_distr, reduction='sum') / (torch.sum(torch.dot(x, torch.log(x))) + torch.sum(torch.dot(y, torch.log(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.2771, dtype=torch.float64)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.kl_div(joint_distr, xy_distr, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-12, 1.0000e-12, 1.0000e-12, 2.0000e-01, 1.0000e-12, 1.0000e-12,\n",
       "        2.0000e-01, 4.0000e-01, 2.0000e-01, 1.0000e-12], dtype=torch.float64)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
